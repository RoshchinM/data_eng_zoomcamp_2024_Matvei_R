{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b0530c-a2e8-42ba-947b-d7661e498985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL веб-интерфейса Spark UI: http://EPUSPRIW0A14.attlocal.net:4040\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание сессии Spark\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Получение URL веб-интерфейса Spark UI\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(\"URL веб-интерфейса Spark UI:\", ui_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe653429-f2cb-4a92-9bab-95e464a7d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e58175-40c3-4ebe-8ea5-1cd756fe1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a4bb8e-b0d5-48c4-b73b-23cb96cd735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4; python_version < \"3.11\" in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff88995-e10b-4b16-bb7f-7973d8aeccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\matvei_roshchin\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f5d2e9-2016-4633-bd6c-e4d4c7d30f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл fhv_tripdata_2019-10.csv.gz успешно распакован в fhv_tripdata_2019-10.csv.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "# Замените 'file.gz' на путь к вашему файлу .gz\n",
    "input_file = 'fhv_tripdata_2019-10.csv.gz'\n",
    "\n",
    "# Создание объекта gzip для чтения сжатых данных\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    # Чтение сжатых данных и преобразование их в DataFrame с помощью pandas\n",
    "    df = pd.read_csv(f_in)\n",
    "\n",
    "# Замените 'output.csv' на имя файла, в который вы хотите сохранить распакованные данные\n",
    "output_file = 'fhv_tripdata_2019-10.csv'\n",
    "\n",
    "# Сохранение DataFrame в формате .csv\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Файл {input_file} успешно распакован в {output_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4479f8a8-8053-4ffd-929b-7148a3c165ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк в файле CSV: 1897493\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Чтение файла CSV\n",
    "df = pd.read_csv(\"fhv_tripdata_2019-10.csv\")\n",
    "\n",
    "# Подсчет строк\n",
    "num_rows = len(df)\n",
    "print(\"Количество строк в файле CSV:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd793c2-c72b-48c0-9f63-3c50efd3644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bd9f85-a0b4-48eb-a669-1e0575b82590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c2c706-27c9-4f04-bcac-cd7418e76705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 1001 строк успешно сохранены в файле head.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Чтение первых 100 строк из файла CSV\n",
    "df_head = pd.read_csv(\"fhv_tripdata_2019-10.csv\", nrows=1001)\n",
    "\n",
    "# Сохранение этих строк в другой файл CSV\n",
    "df_head.to_csv(\"head_fhv.csv\", index=False)\n",
    "\n",
    "print(\"Первые 1001 строк успешно сохранены в файле head.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0b0989-bd44-4d8e-bd62-b3abc7fc203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2aa344-944c-40ff-a632-30b3b2f6aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head_fhv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b62586-55fd-41e4-9198-10376a722da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ea23e89-a9ee-4072-a2c8-34e26c83abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "   pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24692e72-9ee7-4197-9e29-16ea0865978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Определение схемы с учетом типов данных из вашего описания\n",
    "schema = StructType([\n",
    "    StructField(\"dispatching_base_num\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropOff_datetime\", TimestampType(), True),\n",
    "    StructField(\"PUlocationID\", DoubleType(), True),\n",
    "    StructField(\"DOlocationID\", DoubleType(), True),\n",
    "    StructField(\"SR_Flag\", DoubleType(), True),\n",
    "    StructField(\"Affiliated_base_number\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b67fe7a-51bc-4840-8ed3-e74c6f698497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas['Affiliated_base_number'] = df_pandas['Affiliated_base_number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba3ce35c-db86-438b-8698-7f7692ef1fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9897ab96-1e58-4c5a-b900-c45ae1a8799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c95c2d87-2de8-45e7-8669-138eafd51e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "types.StructField('pickup_datetime', types.StringType(), True), \n",
    "types.StructField('dropOff_datetime', types.StringType(), True), \n",
    "types.StructField('PUlocationID', types.StringType(), True), \n",
    "types.StructField('DOlocationID', types.StringType(), True), \n",
    "types.StructField('SR_Flag', types.TimestampType(), True), \n",
    "types.StructField('Affiliated_base_number', types.TimestampType(), True)\n",
    "] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387184fe-dafc-4e87-936e-72ed3057e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d79e1cd4-564a-42a5-805a-9ee32109de4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2019-10-01 00:23:00', dropOff_datetime='2019-10-01 00:35:00', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2019-10-01 00:11:29', dropOff_datetime='2019-10-01 00:13:22', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:11:43', dropOff_datetime='2019-10-01 00:37:20', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:56:29', dropOff_datetime='2019-10-01 00:57:47', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:23:09', dropOff_datetime='2019-10-01 00:28:27', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime='2019-10-01 00:00:48', dropOff_datetime='2019-10-01 00:07:12', PUlocationID='129.0', DOlocationID='129.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime='2019-10-01 00:47:23', dropOff_datetime='2019-10-01 00:53:25', PUlocationID='57.0', DOlocationID='57.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime='2019-10-01 00:10:06', dropOff_datetime='2019-10-01 00:19:50', PUlocationID='173.0', DOlocationID='173.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime='2019-10-01 00:51:37', dropOff_datetime='2019-10-01 01:06:14', PUlocationID='226.0', DOlocationID='226.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00021         ', pickup_datetime='2019-10-01 00:28:23', dropOff_datetime='2019-10-01 00:34:33', PUlocationID='56.0', DOlocationID='56.0', SR_Flag=None, Affiliated_base_number=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0b6698a-0cc5-4b95-9610-f3463dd85765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcec3641-fb60-4b9d-9a54-dd6075553e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)\n",
    "df = df.coalesce(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35e75a16-3153-4dd7-a3bf-07664ce730b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71a22209-3e42-4148-90c8-09114ad9c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhv1/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d8031f-1688-4ac4-a654-c96550b778e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv1/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6be9b0d9-5295-4229-8a84-941e971bc7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: timestamp (nullable = true)\n",
      " |-- Affiliated_base_number: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e105eca7-256c-4b44-af3a-5315057c27eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00037', pickup_datetime='2019-10-02 00:41:42', dropOff_datetime='2019-10-02 00:58:19', PUlocationID='264.0', DOlocationID='177.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B02509', pickup_datetime='2019-10-04 10:35:05', dropOff_datetime='2019-10-04 11:00:37', PUlocationID='264.0', DOlocationID='74.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00837', pickup_datetime='2019-10-02 06:53:34', dropOff_datetime='2019-10-02 07:32:46', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B02803', pickup_datetime='2019-10-01 10:00:23', dropOff_datetime='2019-10-01 10:04:12', PUlocationID='78.0', DOlocationID='169.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B01239', pickup_datetime='2019-10-01 10:05:50', dropOff_datetime='2019-10-01 10:17:22', PUlocationID='264.0', DOlocationID='265.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00821', pickup_datetime='2019-10-03 21:17:38', dropOff_datetime='2019-10-03 21:36:38', PUlocationID='264.0', DOlocationID='260.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B01087', pickup_datetime='2019-10-04 05:55:43', dropOff_datetime='2019-10-04 06:24:02', PUlocationID='230.0', DOlocationID='1.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B00972', pickup_datetime='2019-10-02 09:39:07', dropOff_datetime='2019-11-03 10:03:03', PUlocationID='84.0', DOlocationID='214.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B01623', pickup_datetime='2019-10-02 06:02:00', dropOff_datetime='2019-10-02 06:10:00', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B01730', pickup_datetime='2019-10-03 19:26:22', dropOff_datetime='2019-10-03 19:34:44', PUlocationID='264.0', DOlocationID='232.0', SR_Flag=None, Affiliated_base_number=None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69d752ce-4e28-4d06-af3a-22afec3bc958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on 15th October: 62610\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Преобразование столбцов pickup_datetime и dropOff_datetime в формат времени\n",
    "df = df.withColumn('pickup_datetime', df['pickup_datetime'].cast('timestamp'))\n",
    "df = df.withColumn('dropOff_datetime', df['dropOff_datetime'].cast('timestamp'))\n",
    "\n",
    "# Фильтрация строк по дате 15 октября\n",
    "trips_on_15th_oct = df.filter((col(\"pickup_datetime\").cast(\"date\") == \"2019-10-15\"))\n",
    "\n",
    "# Подсчет количества поездок\n",
    "num_trips_on_15th_oct = trips_on_15th_oct.count()\n",
    "\n",
    "print(\"Number of taxi trips on 15th October:\", num_trips_on_15th_oct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f1686ae-07f7-4f16-9834-4f0c8454a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|pickup_date|max_trip_duration_hours|\n",
      "+-----------+-----------------------+\n",
      "| 2019-10-05|      698.1808333333333|\n",
      "| 2019-10-24|      243.1236111111111|\n",
      "| 2019-10-01|      70128.02805555555|\n",
      "| 2019-10-22|                 290.22|\n",
      "| 2019-10-04|      745.6166666666667|\n",
      "| 2019-10-02|      770.2313888888889|\n",
      "| 2019-10-08|      626.0822222222222|\n",
      "| 2019-10-30|     1465.5344444444445|\n",
      "| 2019-10-14|     483.03833333333336|\n",
      "| 2019-10-21|      314.8233333333333|\n",
      "| 2019-10-28|               631152.5|\n",
      "| 2019-10-25|     1057.8266666666666|\n",
      "| 2019-10-26|      8784.166666666666|\n",
      "| 2019-10-15|     458.24194444444447|\n",
      "| 2019-10-12|               529.9125|\n",
      "| 2019-10-20|                338.225|\n",
      "| 2019-10-31|      87672.44083333333|\n",
      "| 2019-10-10|      578.3888888888889|\n",
      "| 2019-10-17|                 8794.0|\n",
      "| 2019-10-16|      605.0666666666667|\n",
      "+-----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Разница между датой и временем начала и окончания путешествия\n",
    "df = df.withColumn(\"trip_duration_hours\", (F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\")) / 3600)\n",
    "\n",
    "# Извлечение даты из pickup_datetime\n",
    "df = df.withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\"))\n",
    "\n",
    "# Группировка по дате и поиск максимальной продолжительности путешествия для каждого дня\n",
    "max_trip_duration_per_day = df.groupBy(\"pickup_date\").agg(F.max(\"trip_duration_hours\").alias(\"max_trip_duration_hours\"))\n",
    "\n",
    "# Вывод результата\n",
    "max_trip_duration_per_day.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fb99b52-dbc1-48f7-9539-f20387e98129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое длинное путешествие в день: 631152.5 часов\n"
     ]
    }
   ],
   "source": [
    "# Нахождение самого длинного путешествия\n",
    "longest_trip = max_trip_duration_per_day.orderBy(F.desc(\"max_trip_duration_hours\")).first()\n",
    "\n",
    "# Вывод результата\n",
    "longest_trip_duration_hours = longest_trip[\"max_trip_duration_hours\"]\n",
    "print(\"Самое длинное путешествие в день:\", longest_trip_duration_hours, \"часов\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c845e040-e4c5-4144-b03a-e5b2a6b0b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PUlocationID|count|\n",
      "+------------+-----+\n",
      "|         2.0|    1|\n",
      "+------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Группировка по PUlocationID и подсчет числа поездок\n",
    "trip_counts_by_location = df.groupBy('PUlocationID').count()\n",
    "\n",
    "# Сортировка по убыванию числа поездок\n",
    "sorted_trip_counts = trip_counts_by_location.orderBy(F.asc(\"count\"))\n",
    "\n",
    "# Вывод первой строки с самым большим количеством поездок\n",
    "sorted_trip_counts.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88204a1f-671c-4ef4-b79d-beb9b5a48984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
